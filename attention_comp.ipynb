{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WVDvLnmH1Feq",
        "oqgzsgkD1jCc",
        "ln8eJScU4OXb",
        "QcSjuy-O8_ly",
        "pintxI8ASIVP",
        "GqG_b3n8SM64",
        "hevf55qUSQ4I",
        "SNRjr1B3YR3L",
        "fXFP2lAqqByq",
        "aWsV08Enrcn0",
        "J_cCCyxbrvY-",
        "lsv-f--XOLhd",
        "uY2BbfPJ46YQ",
        "3AmTie3-5BqZ",
        "DEY6oNkS5V_a",
        "LIDPY51Z5_vC",
        "UdFC2G-nvqh5",
        "mYuipGKcv31q"
      ],
      "authorship_tag": "ABX9TyMXZA6VtdfBGkE4DsnBsHyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliemci/building-LLM/blob/main/attention_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanism"
      ],
      "metadata": {
        "id": "g2u_gB45pXuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN work for translatting short sentences but fail for longer texts as they don't have direct access to previous words in the input. *Bahdanau attention* mechanism for RNNs modifies the decoder to have selective access. to differeent parts of the input sequence at each decoding step. **Self-attention** allows each position in the input sequence to access all positions in the same sequence when computing the representation of a sequence"
      ],
      "metadata": {
        "id": "6Tv_EFFEOVo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "bVcEvoQYvJ7p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul7yKrrnpSvT",
        "outputId": "2fb88a18-d3e2-43ad-a030-241fbeb459f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/LLM\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEDANBiXph9w",
        "outputId": "62d599da-72d3-4082-d1ec-b1f86fae54d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/LLM\n",
            "attention_comp.ipynb  sliding_window_sampling.ipynb  token_embedding.ipynb\n",
            "pytorch_wormup.ipynb  the-verdict.txt                tokenizing_text.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJEFgXuEingc",
        "outputId": "d065568f-0d46-46c7-a18f-dfe24adcf156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "nLdk-EHipopF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text tokenization and embedding"
      ],
      "metadata": {
        "id": "6Vo0-2O68AbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate BPE tokenizer from tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "\n",
        "\n",
        "context_size = 6\n",
        "context = enc_text[:context_size]\n",
        "\n",
        "print(f\"tokenized input text:\\n{context}\")\n",
        "print(f\"detokenizezd text:\\n{tokenizer.decode(context)}\")"
      ],
      "metadata": {
        "id": "Voa5TTm33hmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b600fb-8bf8-40a5-f31a-3cb98a234b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized input text:\n",
            "[40, 367, 2885, 1464, 1807, 3619]\n",
            "detokenizezd text:\n",
            "I HAD always thought Jack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_dict = {k.strip() : v  for k,v in zip(raw_text[:context_size].split(), context)}\n",
        "\n",
        "for k in list(word_token_dict.keys())[:context_size]:\n",
        "  print(f\"{k} : {word_token_dict[k]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4m9i4xpS5Hw",
        "outputId": "07f00953-580e-4a47-de00-8a4d52f84057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I : 40\n",
            "HAD : 367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 5\n",
        "# max length of input text\n",
        "# suported input size of the LLM is\n",
        "# context_length = max_length\n",
        "\n",
        "# embedding each tocken into a output_dim - dimensional vector\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "#print(f\"randomly initialized weight matrix:\\n{embedding_layer.weight}\")\n",
        "token_embeddings = embedding_layer(torch.tensor(context, dtype=torch.int))\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "id": "WYnS-ZEs7oJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117c38bc-790d-4477-d005-202fa4a81339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533],\n",
            "        [ 2.0478,  1.8619, -1.4766, -1.4558, -0.5568],\n",
            "        [ 0.4197,  0.6117, -0.2094,  0.9823,  0.9884],\n",
            "        [ 1.1062, -0.5667, -1.8651,  0.3535,  2.5554],\n",
            "        [-0.3660,  1.7561,  0.8017,  0.9675,  1.7021],\n",
            "        [-1.1976, -1.5655, -1.2657, -0.3559, -0.7629]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = token_embeddings\n",
        "query = inputs[0]\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "  print(f\"token number {i} with {output_dim}-dim embeding\\n{x_i.data}\")\n",
        "  print(f\"its context scores to the first word 'I':\\n{torch.dot(x_i, query)}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "L645ePwXs0CA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ba3769-fd22-418a-d699-d9d7b0874ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token number 0 with 5-dim embeding\n",
            "tensor([ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533])\n",
            "its context scores to the first word 'I':\n",
            "3.1400835514068604\n",
            "\n",
            "token number 1 with 5-dim embeding\n",
            "tensor([ 2.0478,  1.8619, -1.4766, -1.4558, -0.5568])\n",
            "its context scores to the first word 'I':\n",
            "0.622178852558136\n",
            "\n",
            "token number 2 with 5-dim embeding\n",
            "tensor([ 0.4197,  0.6117, -0.2094,  0.9823,  0.9884])\n",
            "its context scores to the first word 'I':\n",
            "1.947705626487732\n",
            "\n",
            "token number 3 with 5-dim embeding\n",
            "tensor([ 1.1062, -0.5667, -1.8651,  0.3535,  2.5554])\n",
            "its context scores to the first word 'I':\n",
            "4.986784934997559\n",
            "\n",
            "token number 4 with 5-dim embeding\n",
            "tensor([-0.3660,  1.7561,  0.8017,  0.9675,  1.7021])\n",
            "its context scores to the first word 'I':\n",
            "0.7866894602775574\n",
            "\n",
            "token number 5 with 5-dim embeding\n",
            "tensor([-1.1976, -1.5655, -1.2657, -0.3559, -0.7629])\n",
            "its context scores to the first word 'I':\n",
            "-1.1858032941818237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Self-Attention"
      ],
      "metadata": {
        "id": "i0MEcqJSaSsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention computes attention weights by relating. different. positions wirhin a single input sequence."
      ],
      "metadata": {
        "id": "3aHu3viWQ7kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context vector for a query"
      ],
      "metadata": {
        "id": "hBuYMm-t7sBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention scores wrt a query"
      ],
      "metadata": {
        "id": "WVDvLnmH1Feq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scalar value of the sum of the element-wise multiplication of the every element of the sequence with th query is their dot product which is a measure of similarity as it quantifies how much two vectors are aligned."
      ],
      "metadata": {
        "id": "LdY0cMQP0bJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the intemediate attention scores between the query,\n",
        "# the first word in the sequence with every other input token\n",
        "# as the dot product of their embeding coordinates\n",
        "attn_scores_0 = torch.empty(inputs.shape[0])\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_0[i] = torch.dot(x_i, query)\n",
        "\n",
        "print(f\"second word attention score:\\n{attn_scores_0}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBoCZUR7qkSG",
        "outputId": "a71900b1-ae0b-4b7c-b5ff-d237c0d8d8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "second word attention score:\n",
            "tensor([ 3.1401,  0.6222,  1.9477,  4.9868,  0.7867, -1.1858,  4.0621, -1.7895,\n",
            "         3.2612,  0.4932, -0.8053, -1.4909, -0.9968,  0.3160, -0.7135, -0.8053,\n",
            "        -2.5393,  1.2947, -1.5226,  0.3160], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-mMWJH-71L99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention Weights wrt a query"
      ],
      "metadata": {
        "id": "oqgzsgkD1jCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain attention weights, normalize attention scores with sum up to 1 or using softmax, which is better at managing extreme values and offers more favorable gradient properties during training. Softmax function ensures that the attention weights are always positive. The output interpretable as probabilities or relative importance, where higher weights indicate greater importance."
      ],
      "metadata": {
        "id": "IfIWGySV1oPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain attention weights by normalizing the attention scores,\n",
        "# so that the weights sup up to 1:\n",
        "attn_weights_0_tmp = attn_scores_0 / attn_scores_0.sum()\n",
        "print(\"Attention weights from scores normalized with sum:\\n\", attn_weights_0_tmp)\n",
        "print(\"Sum:\", attn_weights_0_tmp.sum())\n",
        "\n",
        "# or by using softmax function\n",
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "# pytorch implementation of softmax, optimize for paerformance\n",
        "attn_weights_0_naive = torch.softmax(attn_scores_0, dim=0)\n",
        "print(\"Attention weights from scores with softmax:\", attn_weights_0_naive)\n",
        "print(\"Sum:\", attn_weights_0_naive.sum())\n",
        "\n",
        "# context vector for the second word is calculated as a weighted sum\n",
        "# of all input vectors\n",
        "context_vec_0 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "    context_vec_0 += attn_weights_0_naive[i]*x_i\n",
        "\n",
        "print(f\"context vector for second word:\\n{context_vec_0}\")"
      ],
      "metadata": {
        "id": "7DIaCBsy2MLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6c3b29-5787-4588-c3ad-7645221966ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights from scores normalized with sum:\n",
            " tensor([ 0.3349,  0.0663,  0.2077,  0.5318,  0.0839, -0.1265,  0.4332, -0.1908,\n",
            "         0.3478,  0.0526, -0.0859, -0.1590, -0.1063,  0.0337, -0.0761, -0.0859,\n",
            "        -0.2708,  0.1381, -0.1624,  0.0337], grad_fn=<DivBackward0>)\n",
            "Sum: tensor(1., grad_fn=<SumBackward0>)\n",
            "Attention weights from scores with softmax: tensor([8.3840e-02, 6.7599e-03, 2.5445e-02, 5.3145e-01, 7.9687e-03, 1.1085e-03,\n",
            "        2.1079e-01, 6.0610e-04, 9.4632e-02, 5.9422e-03, 1.6219e-03, 8.1700e-04,\n",
            "        1.3391e-03, 4.9768e-03, 1.7776e-03, 1.6219e-03, 2.8635e-04, 1.3243e-02,\n",
            "        7.9154e-04, 4.9768e-03], grad_fn=<SoftmaxBackward0>)\n",
            "Sum: tensor(1., grad_fn=<SumBackward0>)\n",
            "context vector for second word:\n",
            "tensor([ 1.1629, -0.4412, -1.4258,  0.2621,  1.9211], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context Vector for a query"
      ],
      "metadata": {
        "id": "ln8eJScU4OXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The context vector z_0 is calculated by multiplying the embedded input tokens, x_i, with the corresponding attention weights and then summing the resulting vectors, i.e. **the weighted sum of all imput vectors**"
      ],
      "metadata": {
        "id": "ufKboLLJ5Wmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_0 = torch.zeros(query.shape)\n",
        "\n",
        "for i,x_i in enumerate(inputs):\n",
        "    context_vec_0 += attn_weights_0_naive[i]*x_i\n",
        "print(context_vec_0)"
      ],
      "metadata": {
        "id": "uzkXpnRf8quB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f583e6ce-de91-4bcd-a702-a5775044531e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.1629, -0.4412, -1.4258,  0.2621,  1.9211], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context vectors for all input tokens"
      ],
      "metadata": {
        "id": "QcSjuy-O8_ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention Scores"
      ],
      "metadata": {
        "id": "pintxI8ASIVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention score bw each pair of inputs\n",
        "attn_scores = torch.empty(inputs.shape[0], inputs.shape[0])\n",
        "\n",
        "# with add an additional for-loop to compute the dot products\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "# matrix multiplication musch faster than for-loop\n",
        "attn_scores = inputs @ inputs.T\n",
        "\n",
        "print(f\"All pairs attention scores:\\n{attn_scores}\")"
      ],
      "metadata": {
        "id": "xM1dudBgSe8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention Weights"
      ],
      "metadata": {
        "id": "GqG_b3n8SM64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize each input element attention scores so that they sum up to 1\n",
        "attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "\n",
        "print(f\"All rowa sum to 1:{attn_weights.sum(dim=1)}\")\n",
        "print(f\" Attention weights:\\n{attn_weights}\")"
      ],
      "metadata": {
        "id": "HBQSmkXvUyi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Context Vectors"
      ],
      "metadata": {
        "id": "hevf55qUSQ4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(f\"All context vectors:\\n{all_context_vecs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH3DOxGqWBDv",
        "outputId": "e29aa562-12e4-4215-c72e-b663a4b1d22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All context vectors:\n",
            "tensor([[ 1.1629, -0.4412, -1.4258,  0.2621,  1.9211],\n",
            "        [ 2.0427,  1.8600, -1.4764, -1.4530, -0.5568],\n",
            "        [ 0.5325,  0.4804, -0.4147,  0.5660,  1.4415],\n",
            "        [ 1.1028, -0.5601, -1.8310,  0.3006,  2.4950],\n",
            "        [-0.1962,  1.6651,  0.9834,  0.9394,  1.5002],\n",
            "        [-1.1453, -1.4533, -1.2700, -0.3885, -0.6535],\n",
            "        [ 1.1886, -0.5651, -1.6610,  0.1374,  2.3083],\n",
            "        [ 1.0457,  0.2737, -0.2043, -0.5802, -2.0543],\n",
            "        [ 1.0403, -0.3959, -1.8066,  0.3355,  2.0152],\n",
            "        [-0.8372, -0.0726, -2.0837, -1.6954,  1.9354],\n",
            "        [-0.0320,  0.7286,  1.4154,  0.5579,  0.0824],\n",
            "        [-0.5520,  0.4166,  0.0678, -0.0263,  0.3155],\n",
            "        [ 1.0582, -0.0538,  0.1632, -0.2290, -1.9805],\n",
            "        [ 1.1630,  1.2001,  2.8794,  0.8324,  0.0437],\n",
            "        [-0.4042, -0.0782,  0.2316,  0.1970, -0.3493],\n",
            "        [-0.0320,  0.7286,  1.4154,  0.5579,  0.0824],\n",
            "        [ 0.2798,  0.7391,  2.6261,  0.6391, -0.6024],\n",
            "        [ 1.7956, -0.1916,  0.1868, -0.1356, -1.4490],\n",
            "        [-1.1766,  2.2510, -2.3363, -0.2483,  0.0844],\n",
            "        [ 1.1630,  1.2001,  2.8794,  0.8324,  0.0437]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"previousl calculated context_vec_0:\\n{context_vec_0}\")\n",
        "print(f\"\\nFirst row of all contet vectors tensor:\\n{all_context_vecs[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwgm62j4Wso8",
        "outputId": "26f91b0a-b564-4ae0-bb90-34980892df1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "previousl calculated context_vec_0:\n",
            "tensor([ 1.1629, -0.4412, -1.4258,  0.2621,  1.9211], grad_fn=<AddBackward0>)\n",
            "\n",
            "First row of all contet vectors tensor:\n",
            "tensor([ 1.1629, -0.4412, -1.4258,  0.2621,  1.9211],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-attention with trainable weights"
      ],
      "metadata": {
        "id": "SNRjr1B3YR3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention scores wrt quiery"
      ],
      "metadata": {
        "id": "fXFP2lAqqByq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"query is the first sequence token: {inputs[0]}\")\n",
        "print(f\"input sequence consists of {inputs.shape[0]} tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqrp-xIbeAnI",
        "outputId": "162e30af-ecf0-462e-da46-e0014a0ffd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query is the first sequence token: tensor([ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "input sequence consists of 13 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set a quiery to the first word in text\n",
        "x_0 = inputs[0]\n",
        "\n",
        "d_in = 5\n",
        "d_out = 5"
      ],
      "metadata": {
        "id": "Z-RZbsqhsddA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize weight matricess Wq, Wk, and Wv;\n",
        "# use requires_grad=False for less clutered output, =True for model training\n",
        "torch.manual_seed(123)\n",
        "\n",
        "Wq = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "Wk  = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "Wv = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "Jhyt_-KXtFbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three trainable weight matrices Wq, Wk and Wv are used to project an embedded input token into query, key and value vectors. The terms **key**, **query** and **value** are borrowed from information retrival and databases:\n",
        "\n",
        "* query is used to probe the other parts of the input sequence to determine how much attention to pay to them\n",
        "\n",
        "* key is used for indexing and serching, each item in the input sequence has an associated key\n",
        "\n",
        "* value is representation of the input items\n",
        "\n",
        "When the model determines which keys and thus which parts of the input are most relevant to the query, the current focus item, it retrieves the corresponding values"
      ],
      "metadata": {
        "id": "c0CkPOzOaC9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb5lq410g3gu",
        "outputId": "6a35094b-9411-417b-e89c-45ef717e5f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute query, key and value vectors for the first token\n",
        "query_0 = x_0 @ Wq\n",
        "key_0 = x_0 @ Wk\n",
        "value_0 = x_0 @ Wv\n",
        "\n",
        "print(query_0)"
      ],
      "metadata": {
        "id": "LeblRfiwt_xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5707ee-2c10-48ed-a05f-5c038cad8c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9389, 0.9168, 1.4128, 1.8544, 0.3706], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute all keys and values\n",
        "keys = inputs @ Wk\n",
        "values = inputs @ Wv\n",
        "\n",
        "# project the first token from d_in to d_out embeding space\n",
        "print(\"all keys shape:\", keys.shape)\n",
        "print(\"all values shape:\", values.shape)"
      ],
      "metadata": {
        "id": "kMpUo8evuUxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747fa00c-21ed-4530-bf6f-fb56e8e5d35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all keys shape: torch.Size([13, 5])\n",
            "all values shape: torch.Size([13, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the attention score for query\n",
        "keys_0 = keys[0]\n",
        "attn_score_00 = query_0.dot(keys_0)\n",
        "\n",
        "print(f\"unormalized attention for the first element: {attn_score_00}\")"
      ],
      "metadata": {
        "id": "okytZGoFvejQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cd4f358-da56-42fa-bcf5-3ce67d0950ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unormalized attention for the first element: 7.827261924743652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute all attention scores with matrix mutiplication\n",
        "attn_scores_0 = query_0 @ keys.T # All attention scores for given query\n",
        "\n",
        "print(attn_scores_0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTOYrc73j-UV",
        "outputId": "6830acee-74fc-4934-a4ff-7b49f05eb63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  7.8273,   6.6440,  10.7050,   9.7635,  16.6555, -16.9381,   7.2245,\n",
            "         -7.0322,   5.7098,  -4.8935,   1.2303,  -3.0847,  -5.4073],\n",
            "       grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention weights"
      ],
      "metadata": {
        "id": "aWsV08Enrcn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight parameters are the fundamental, learned coefficients that define the network's connections, while attention weights are dynamic, context-specific values.\n",
        "\n",
        "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients, which can slow down learning."
      ],
      "metadata": {
        "id": "Nq5ZJJgKie65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the attention scores by dividing them by the square toot (**0.5)\n",
        "# of the embedding dimension of the keys\n",
        "d_k = keys.shape[-1]\n",
        "\n",
        "attn_weights_0 = torch.softmax(attn_scores_0 / d_k**0.5, dim=-1)\n",
        "print(f\"attention weights for the first element in input sequence:\\n{attn_weights_0}\")"
      ],
      "metadata": {
        "id": "L_RNgRACw29_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc58ae50-2705-4f25-baa8-6115babf82fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention weights for the first element in input sequence:\n",
            "tensor([1.6490e-02, 9.7141e-03, 5.9721e-02, 3.9199e-02, 8.5478e-01, 2.5540e-07,\n",
            "        1.2593e-02, 2.1437e-05, 6.3965e-03, 5.5791e-05, 8.6285e-04, 1.2528e-04,\n",
            "        4.4338e-05], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context Vector"
      ],
      "metadata": {
        "id": "J_cCCyxbrvY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the context vector as a weighted sum over the value vectors\n",
        "context_vec_0 = attn_weights_0 @ values\n",
        "\n",
        "print(context_vec_0)"
      ],
      "metadata": {
        "id": "ALfQx5rOxlX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e811a518-b9a8-4732-ee23-3a85f3e22d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2.2702, 2.3605, 1.9509, 0.9412, 2.0605], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-attention class"
      ],
      "metadata": {
        "id": "lsv-f--XOLhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = token_embeddings\n",
        "\n",
        "d_in = 5\n",
        "d_out = 5"
      ],
      "metadata": {
        "id": "lqwMPsN6S5ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v1(nn.Module):\n",
        "  \"\"\" Class which implements self-attention mechanism, by\n",
        "  transformation of the input matrix X with the three weight\n",
        "  matrices, Wq, Wk and Wv, which are initialized for queries,\n",
        "  key and values, and each of which transforms the input dimesion d_in to an\n",
        "  output dimension d_out. The forward method computes the\n",
        "  attention scores by multiplying queries and keys,\n",
        "  normalizing using softmax to get attention weights and\n",
        "  creating a context vector by weighting the values with\n",
        "  attention weights. \"\"\"\n",
        "\n",
        "  def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.Wq = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.Wk = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.Wv = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "        keys = x @ self.Wk\n",
        "        queries = x @ self.Wq\n",
        "        values = x @ self.Wv\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "bDjuIBsjOer4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(f\"the context vectors for the inputs embeded vectors:\\n{sa_v1(inputs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fCrgUjIwqgw",
        "outputId": "a23cb548-8458-43c1-e3f4-1c7fb0d37e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the context vectors for the inputs embeded vectors:\n",
            "tensor([[ 2.2702,  2.3605,  1.9509,  0.9412,  2.0605],\n",
            "        [ 0.2346, -0.0417,  0.2971,  0.1990,  0.2329],\n",
            "        [ 2.3596,  2.5487,  2.0782,  0.9772,  2.1461],\n",
            "        [ 2.3313,  2.4761,  2.0233,  0.9595,  2.1185],\n",
            "        [ 2.3786,  2.5853,  2.1017,  0.9847,  2.1628],\n",
            "        [-2.5004, -2.9339, -2.4945, -1.9440, -1.8519],\n",
            "        [ 2.1642,  2.1103,  1.7717,  0.8904,  1.9541],\n",
            "        [-2.4981, -2.9302, -2.4886, -1.9387, -1.8508],\n",
            "        [ 1.9528,  1.7526,  1.5367,  0.8310,  1.7487],\n",
            "        [-1.8738, -2.1373, -1.4870, -1.0815, -1.4365],\n",
            "        [ 1.5137,  1.3669,  1.3375,  0.7416,  1.3573],\n",
            "        [-1.1438, -1.3443, -0.7745, -0.5407, -0.9008],\n",
            "        [-2.4802, -2.9036, -2.4500, -1.9045, -1.8412]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "  \"\"\" Self-attention mechanism class with an optimized weight\n",
        "  initialization with nn.Linear, which leads to more stable\n",
        "  and efective model training. \"\"\"\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.Wq = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.Wk = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.Wv = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "        keys = self.Wk(x)\n",
        "        queries = self.Wq(x)\n",
        "        values = self.Wv(x)\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "        context_vec = attn_weights @ values\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "PwQDRzq2y0sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(f\"inputs context vectors:\\n{sa_v2(inputs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOrx5E1Hzxg8",
        "outputId": "52529c5a-96cb-4cfb-f237-ea6391967e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs context vectors:\n",
            "tensor([[ 0.0805, -0.3446, -0.4164,  0.3569,  0.2441],\n",
            "        [ 0.2634, -0.3499, -0.3964,  0.5085,  0.3916],\n",
            "        [ 0.1661, -0.3426, -0.4020,  0.4278,  0.3110],\n",
            "        [ 0.0424, -0.4120, -0.3911,  0.3255,  0.2813],\n",
            "        [ 0.2608, -0.3202, -0.3885,  0.5088,  0.3700],\n",
            "        [ 0.1498, -0.3763, -0.2494,  0.3326,  0.3329]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** nn.Linear in SelfAttention_v2 uses a different weight initialization scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes both mechanisms to produce different results."
      ],
      "metadata": {
        "id": "VLdU3qOU1fOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal Attention"
      ],
      "metadata": {
        "id": "7Q1NVThewJ3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causal Attention is a specialized form of self-attention that restircts a model to only consider previous and current inputs in a sequence when processing any given token."
      ],
      "metadata": {
        "id": "KOg89rB6wszQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Mask"
      ],
      "metadata": {
        "id": "d-IqMNOtvkHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each token processed, the future tokens, which come after the current token in the input text are masked. Thus the LLM cant access future tokens when computing the context vectors using attention weights. This is implemented masking out the attention weights above the diagonal, by applying the softmax function to the attentions cores, zeroing out the elemnts above the diagonal and normalizing the  the non-masked attention weights such that the attention weights sum up to 1 in each row"
      ],
      "metadata": {
        "id": "d1RkqF0PxXqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute Attention weights"
      ],
      "metadata": {
        "id": "uY2BbfPJ46YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute attention weights using softmax function\n",
        "queries = sa_v2.Wq(inputs)\n",
        "keys = sa_v2.Wk(inputs)\n",
        "\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "\n",
        "print(f\"normalized attention weights matrix:\\n{attn_weights}\")\n"
      ],
      "metadata": {
        "id": "4zGbWQAMx7Gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7107bb-3745-4b32-ae83-10f4d0ff9a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalized attention weights matrix:\n",
            "tensor([[0.1815, 0.1013, 0.1748, 0.2151, 0.1845, 0.1428],\n",
            "        [0.2505, 0.2075, 0.1711, 0.1324, 0.1057, 0.1327],\n",
            "        [0.1998, 0.1520, 0.1732, 0.1806, 0.1515, 0.1429],\n",
            "        [0.1835, 0.0825, 0.1734, 0.2523, 0.2339, 0.0744],\n",
            "        [0.2101, 0.2091, 0.1644, 0.1409, 0.1125, 0.1630],\n",
            "        [0.1177, 0.1556, 0.1554, 0.1897, 0.2715, 0.1102]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mask out"
      ],
      "metadata": {
        "id": "3AmTie3-5BqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mask where the values above the diagonal are zero\n",
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "\n",
        "print(f\"mask with 0s above the diagonal:\\n{mask_simple}\")"
      ],
      "metadata": {
        "id": "jwjgmDEJ0u17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c0e78b-83f1-4937-a968-1c77fac43d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask with 0s above the diagonal:\n",
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply the mask with attention weights to zero out the values\n",
        "# above the diagonal\n",
        "masked_attn_weights = attn_weights * mask_simple\n",
        "print(f\"(attention weight matrix:\\n{masked_attn_weights}\")"
      ],
      "metadata": {
        "id": "yHY7uCjt1D-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba16ab1-0af4-48c1-9c17-0b2fab26bb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(attention weight matrix:\n",
            "tensor([[0.1815, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2505, 0.2075, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1998, 0.1520, 0.1732, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1835, 0.0825, 0.1734, 0.2523, 0.0000, 0.0000],\n",
            "        [0.2101, 0.2091, 0.1644, 0.1409, 0.1125, 0.0000],\n",
            "        [0.1177, 0.1556, 0.1554, 0.1897, 0.2715, 0.1102]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Renormalize"
      ],
      "metadata": {
        "id": "DEY6oNkS5V_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remormalize the attention weights to sum up to 1 gain in each row\n",
        "row_sums = mask_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = mask_simple / row_sums\n",
        "print(f\"normalized attention weight matrix:\\n{masked_simple_norm}\")"
      ],
      "metadata": {
        "id": "sXSarSqY1xQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74683c07-0309-42ec-e3ce-94c7fac33c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalized attention weight matrix:\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Efficient masking"
      ],
      "metadata": {
        "id": "LIDPY51Z5_vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function converts its inputs into a probability distribution. When -∞ are present in a row, the softmax function treats them as zero probability, since because e-∞ approaches 0. A more efficient masking possible by creating a mask with 1's above the diagonal and then replacing these 1's with -inf values"
      ],
      "metadata": {
        "id": "QH9kVFPa6Kp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "\n",
        "print(f\"mask with -inf values above diagonal:\\n{masked}\")\n",
        "\n",
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "\n",
        "print(f\"\\nnormalized attention weights:n{attn_weights}\")"
      ],
      "metadata": {
        "id": "_W_NBYnq6FS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9956c5d9-47e6-45e8-fae5-f9668bf4e903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mask with -inf values above diagonal:\n",
            "tensor([[ 0.4379,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.7773,  0.3562,    -inf,    -inf,    -inf,    -inf],\n",
            "        [ 0.4701, -0.1414,  0.1503,    -inf,    -inf,    -inf],\n",
            "        [ 0.8943, -0.8932,  0.7666,  1.6058,    -inf,    -inf],\n",
            "        [ 0.4120,  0.4010, -0.1357, -0.4809, -0.9841,    -inf],\n",
            "        [-0.3805,  0.2437,  0.2405,  0.6873,  1.4886, -0.5277]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "\n",
            "normalized attention weights:ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5469, 0.4531, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3806, 0.2895, 0.3299, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2653, 0.1193, 0.2506, 0.3648, 0.0000, 0.0000],\n",
            "        [0.2510, 0.2498, 0.1965, 0.1684, 0.1344, 0.0000],\n",
            "        [0.1177, 0.1556, 0.1554, 0.1897, 0.2715, 0.1102]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weights Dropout"
      ],
      "metadata": {
        "id": "UdFC2G-nvqh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly selected hidden layer units are ignored during training, whcih prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units.\n",
        "Drop outs are apply either after calculating attention score or after appling the attention weights"
      ],
      "metadata": {
        "id": "GiHQaQKB7uRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "# masking out half of the attention weights\n",
        "dropout = nn.Dropout(0.5)\n",
        "\n",
        "print(f\"attention weight matrix with drop outs:\\n{dropout(attn_weights)}\")"
      ],
      "metadata": {
        "id": "iDFPdv9u8w0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f818dd-1cdb-4d92-b19b-23e14375e4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention weight matrix with drop outs:\n",
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.9061, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6598, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5307, 0.2386, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3112, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Causal Attention Class"
      ],
      "metadata": {
        "id": "mYuipGKcv31q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  \"\"\" A self attention class with added dropouts and causel mask. \"\"\"\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "      super().__init__()\n",
        "      self.d_out = d_out\n",
        "      self.Wq = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "      self.Wk = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "      self.Wv = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "      # with the use of register_buffer there is no need to manually\n",
        "      # ensure the tensors are on the same device as the model parameters\n",
        "      self.register_buffer(\n",
        "          'mask',\n",
        "          torch.triu(torch.ones(context_length, context_length),\n",
        "          diagonal=1)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "      b, num_tokens, d_in = x.shape\n",
        "      #new batch dimension b\n",
        "      keys = self.Wk(x)\n",
        "      queries = self.Wq(x)\n",
        "      values = self.Wv(x)\n",
        "\n",
        "      attn_scores = queries @ keys.transpose(1, 2)\n",
        "      attn_scores.masked_fill_(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "      attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "      attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "      context_vec = attn_weights @ values\n",
        "      return context_vec"
      ],
      "metadata": {
        "id": "F-atk95a9NR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to simulate a batch inputs, duplicate the input text\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(f\"{batch.shape[0]} input texts with {batch.shape[1]} token each, where each token is {batch.shape[2]}-dimensional vector:\\n{batch.shape}\")"
      ],
      "metadata": {
        "id": "-ZkKCl6I9IfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846d738d-199a-4f33-f3c4-fd70e406c8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 input texts with 6 token each, where each token is 5-dimensional vector:\n",
            "torch.Size([2, 6, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "print(f\"\\n{context_vecs.shape[2]}-dim context vectors for context_vecs.shape[0] input ssequences of {context_vecs.shape[1]} tokens each:\\n{context_vecs}\")"
      ],
      "metadata": {
        "id": "S_uFOsCz9gDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ddf747-a5e5-4ac3-850e-8b03ec2e0c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 5])\n",
            "\n",
            "5-dim context vectors for context_vecs.shape[0] input ssequences of 6 tokens each:\n",
            "tensor([[[ 0.1682, -0.8474, -0.2907, -0.0228,  0.3905],\n",
            "         [ 0.1450, -1.4204,  1.1329,  0.2614,  1.5192],\n",
            "         [ 0.1730, -1.1179,  0.5192,  0.0413,  0.9298],\n",
            "         [ 0.0618, -1.4199,  0.4697,  0.2299,  1.0985],\n",
            "         [ 0.1228, -1.2190,  0.3619,  0.0986,  0.8890],\n",
            "         [ 0.0015, -0.8100,  0.2812,  0.1129,  0.5850]],\n",
            "\n",
            "        [[ 0.1682, -0.8474, -0.2907, -0.0228,  0.3905],\n",
            "         [ 0.1450, -1.4204,  1.1329,  0.2614,  1.5192],\n",
            "         [ 0.1730, -1.1179,  0.5192,  0.0413,  0.9298],\n",
            "         [ 0.0618, -1.4199,  0.4697,  0.2299,  1.0985],\n",
            "         [ 0.1228, -1.2190,  0.3619,  0.0986,  0.8890],\n",
            "         [ 0.0015, -0.8100,  0.2812,  0.1129,  0.5850]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g5Vn5aYcwDsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention"
      ],
      "metadata": {
        "id": "CFpJcMPyvtx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-head attention consists of stacking multiple instances of causal self-attention mechanisms each with its own weights, and combining their outputs.\n",
        "\n",
        "For example a multi-head attention with two single head attention modules uses two Wv value matrices, two Wq and Wk matrices, obtaining two sets of context vectors which are concatenated into a single context vector matrix, where each context vector is d_out * num_heads-dimensional."
      ],
      "metadata": {
        "id": "SKx662Lwv0-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential multiple single-head attention modules"
      ],
      "metadata": {
        "id": "H69dN1lrR4_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrap(nn.Module):\n",
        "  \"\"\"A wrapper class that stacks multiple instances of the CausalAttention Class,\n",
        "     which are process sequentially with a loop in the forward method\"\"\"\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.head = nn.ModuleList(\n",
        "        [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.head], dim=-1)"
      ],
      "metadata": {
        "id": "GvyRWhdfyP9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = 12\n",
        "inputs = embedding_layer(torch.tensor(context, dtype=torch.int))\n",
        "type(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxK4Yk-hIzwv",
        "outputId": "3bc2ce6a-0a4b-45ec-8fc8-94feac15e34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_size = 12\n",
        "context = enc_text[:context_size]\n",
        "inputs = embedding_layer(torch.tensor(context, dtype=torch.int))\n",
        "\n",
        "# extract two input text sequences of lenth 6 tokens each into a batch\n",
        "max_length = 6\n",
        "\n",
        "first_chunk = inputs[:max_length]\n",
        "second_chunk = inputs[max_length : 2*max_length]\n",
        "batch = torch.stack((first_chunk, second_chunk), dim=0)\n",
        "print(f\"batch of two input sequences:\\n{batch}\")\n",
        "\n",
        "# the number of input tokens\n",
        "context_length = batch.shape[1]\n",
        "\n",
        "d_in, d_out = 5, 5\n",
        "num_heads = 2\n",
        "\n",
        "mha = MultiHeadAttentionWrap(d_in, d_out, context_length, dropout=0.0, num_heads=num_heads)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "print(f\"context vector tensor with {context_vecs.shape[0]} input texts with {context_vecs.shape[1]} token each, where each token is {context_vecs.shape[2]}-dimensional vector:\\n{context_vecs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErzCX1siBWx0",
        "outputId": "c7047821-b6e7-4277-c33c-002aa3e058aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of two input sequences:\n",
            "tensor([[[ 1.1721, -0.4372, -0.4053,  0.7086,  0.9533],\n",
            "         [ 2.0478,  1.8619, -1.4766, -1.4558, -0.5568],\n",
            "         [ 0.4197,  0.6117, -0.2094,  0.9823,  0.9884],\n",
            "         [ 1.1062, -0.5667, -1.8651,  0.3535,  2.5554],\n",
            "         [-0.3660,  1.7561,  0.8017,  0.9675,  1.7021],\n",
            "         [-1.1976, -1.5655, -1.2657, -0.3559, -0.7629]],\n",
            "\n",
            "        [[ 1.6024, -0.6694, -1.0676, -0.3782,  1.8112],\n",
            "         [ 0.8424,  0.2846, -0.2265, -0.7069, -2.3531],\n",
            "         [ 0.9281, -0.1081, -1.9729,  0.7692,  0.8198],\n",
            "         [-0.8608, -0.0757, -2.0866, -1.7262,  1.9371],\n",
            "         [-0.7313,  0.2600,  0.6740,  0.4362,  0.1359],\n",
            "         [-1.0130,  0.1652,  0.3707, -0.4285,  0.2333]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 10])\n",
            "context vector tensor with 2 input texts with 6 token each, where each token is 10-dimensional vector:\n",
            "tensor([[[ 1.6818e-01, -8.4739e-01, -2.9074e-01, -2.2761e-02,  3.9050e-01,\n",
            "           8.2460e-03, -6.0613e-01, -5.5108e-01,  3.8361e-01, -6.8215e-01],\n",
            "         [ 1.4500e-01, -1.4204e+00,  1.1329e+00,  2.6137e-01,  1.5192e+00,\n",
            "          -4.5355e-01,  2.4343e-02, -3.8733e-01, -5.5960e-01, -4.3810e-01],\n",
            "         [ 1.7297e-01, -1.1179e+00,  5.1916e-01,  4.1334e-02,  9.2975e-01,\n",
            "          -8.4925e-02, -2.3210e-01, -3.2667e-01, -9.9180e-02, -6.3073e-01],\n",
            "         [ 6.1778e-02, -1.4199e+00,  4.6969e-01,  2.2993e-01,  1.0985e+00,\n",
            "          -3.7643e-01,  3.1149e-02, -3.8217e-01, -5.8516e-01, -5.1454e-01],\n",
            "         [ 1.2276e-01, -1.2190e+00,  3.6189e-01,  9.8572e-02,  8.8900e-01,\n",
            "           3.5212e-01, -4.0971e-01, -3.5208e-01,  1.6215e-01, -9.2726e-01],\n",
            "         [ 1.5260e-03, -8.1004e-01,  2.8118e-01,  1.1289e-01,  5.8500e-01,\n",
            "          -1.4762e-01, -9.6377e-02, -2.4822e-01, -1.7017e-01, -2.8558e-01]],\n",
            "\n",
            "        [[-9.3273e-02, -1.5547e+00,  1.0814e-01,  6.9522e-01,  1.2568e+00,\n",
            "          -8.1505e-02, -4.4046e-01, -9.1615e-01, -8.2187e-02, -6.5439e-01],\n",
            "         [ 3.9158e-02, -5.4069e-01,  5.3264e-01,  2.1706e-01,  7.3774e-01,\n",
            "          -6.1009e-01, -1.4070e-02, -3.9258e-01, -3.1838e-01,  9.3034e-02],\n",
            "         [-4.9466e-03, -1.0878e+00,  4.0048e-01,  2.6569e-01,  9.1037e-01,\n",
            "          -5.7529e-01, -9.1723e-02, -3.2772e-01, -2.2407e-01, -2.5136e-02],\n",
            "         [-2.8219e-01, -1.0422e+00,  6.2641e-01,  6.2404e-01,  1.0253e+00,\n",
            "          -5.7230e-01,  8.0133e-02, -2.0548e-01, -3.6484e-01,  1.4385e-01],\n",
            "         [-1.1764e-01, -5.5616e-01,  4.3281e-01,  2.5047e-01,  5.5675e-01,\n",
            "          -4.8568e-02, -5.6235e-02, -2.8635e-01, -1.9592e-01, -2.2052e-01],\n",
            "         [-1.5238e-01, -4.2510e-01,  3.8159e-01,  2.8344e-01,  4.6561e-01,\n",
            "          -5.0130e-02,  1.1078e-02, -1.8861e-01, -2.0479e-01, -1.1826e-01]]],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split multy heads attention"
      ],
      "metadata": {
        "id": "i7WxR-DGSELa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" It starts with multi-head layer which gets splits into multiple individual\n",
        "      attention heads by reshaping and transposing query, key and value tesnors,\n",
        "      coumputes the attention for each and then combines the results.\"\"\"\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.Wq = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.Wk = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.Wv = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch, num_tokens, d_in = x.shape\n",
        "    keys = self.Wk(x)\n",
        "    queries = self.Wq(x)\n",
        "    values = self.Wv(x)\n",
        "\n",
        "    # reshape to represent multiple heads, spliting the d_out into\n",
        "    # num_heads = d_out/ head_dim\n",
        "    keys = keys.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(batch, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # transpose to bring the num_heads dim before the num_tokens dim,\n",
        "    # needed for aligning the queries, keys and values across the different\n",
        "    # heads and performing batched matrix multiplication\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # a batched matrix multiplication between the tensor itself and a view of\n",
        "    # the tensor with last two dimentions num_tokens and head_dim being transposed\n",
        "    attn_scores = queries @ keys.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    # the context vectors from all heads are transposed back to the shape (batch, num_tokens, num_heads, head_dim\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "    # to combine the outputs from all heads, the context vectors are reshaped into(batch, num_tokens, d_out)\n",
        "    context_vec = context_vec.contiguous().view(batch, num_tokens, self.d_out)\n",
        "    # adding an output projection layer\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "3oYr98abTzX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(size = [1,2,3,4])\n",
        "\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z-OGj7xeTot",
        "outputId": "67bddc97-8839-4b6c-c5d6-93e12b866b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.6918, 0.3545, 0.7969, 0.0061],\n",
              "          [0.2528, 0.0882, 0.6997, 0.4855],\n",
              "          [0.4067, 0.4168, 0.1092, 0.6418]],\n",
              "\n",
              "         [[0.5125, 0.1549, 0.6881, 0.4900],\n",
              "          [0.0164, 0.7690, 0.7674, 0.4058],\n",
              "          [0.1548, 0.5201, 0.8773, 0.9577]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test batched matrix multiplication with example\n",
        "b = torch.rand(size = [1,2,3,4])\n",
        "\n",
        "print(f\"batch:\\n{b}\")\n",
        "# matrix multiplication between the tensor itself and a view of the tensor\n",
        "# transposing the last two dimensions\n",
        "print(f\"\\nbatch matrix multiplication:\\n{b @ b.transpose(2, 3)}\")\n",
        "\n",
        "# matrix multiplication carried out bw the two last dimensions\n",
        "# of num_tokens and head_dim, and then repeated for individiual heads\n",
        "first_head = b[0, 0, :, :]\n",
        "first_res = first_head @ first_head.T\n",
        "print(\"\\nFirst head:\\n\", first_res)\n",
        "\n",
        "second_head = b[0, 1, :, :]\n",
        "second_res = second_head @ second_head.T\n",
        "print(\"\\nSecond head:\\n\", second_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulbqfwCfcPS9",
        "outputId": "9218d882-45a7-4a4e-be3f-d46e1c846582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch:\n",
            "tensor([[[[0.7239, 0.3604, 0.1829, 0.2956],\n",
            "          [0.8646, 0.8010, 0.8044, 0.0733],\n",
            "          [0.7355, 0.6248, 0.1638, 0.5158]],\n",
            "\n",
            "         [[0.6000, 0.2299, 0.2890, 0.9078],\n",
            "          [0.4596, 0.4947, 0.1836, 0.2010],\n",
            "          [0.9603, 0.6861, 0.4209, 0.8046]]]])\n",
            "\n",
            "batch matrix multiplication:\n",
            "tensor([[[[0.7748, 1.0834, 0.9401],\n",
            "          [1.0834, 2.0415, 1.3059],\n",
            "          [0.9401, 1.3059, 1.2242]],\n",
            "\n",
            "         [[1.3205, 0.6250, 1.5860],\n",
            "          [0.6250, 0.5301, 1.0198],\n",
            "          [1.5860, 1.0198, 2.2175]]]])\n",
            "\n",
            "First head:\n",
            " tensor([[0.7748, 1.0834, 0.9401],\n",
            "        [1.0834, 2.0415, 1.3059],\n",
            "        [0.9401, 1.3059, 1.2242]])\n",
            "\n",
            "Second head:\n",
            " tensor([[1.3205, 0.6250, 1.5860],\n",
            "        [0.6250, 0.5301, 1.0198],\n",
            "        [1.5860, 1.0198, 2.2175]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2 # set so that num_heads = d_out/ head_dim\n",
        "\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(f\"context vectors:\\n{context_vecs}\")\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZYnYFc4gy0l",
        "outputId": "7476a339-fc17-4484-f7a0-1b0bf2b9bfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context vectors:\n",
            "tensor([[[ 0.8227,  0.0246],\n",
            "         [ 0.6647, -0.0818],\n",
            "         [ 0.6566, -0.1010],\n",
            "         [ 0.5320, -0.1808],\n",
            "         [ 0.4877, -0.2188],\n",
            "         [ 0.4077, -0.2375]],\n",
            "\n",
            "        [[ 0.1708, -0.4087],\n",
            "         [ 0.6776, -0.0564],\n",
            "         [ 0.5588, -0.1505],\n",
            "         [ 0.1270, -0.4430],\n",
            "         [ 0.2802, -0.3071],\n",
            "         [ 0.2610, -0.3173]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    }
  ]
}