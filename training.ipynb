{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhvTP0+a9Gash8bO+n3Reb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliemci/building-LLM/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining on Unlabeled Data"
      ],
      "metadata": {
        "id": "rbJI0l4KAAgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt7DBuxV_vZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5931a7-fdeb-49fb-bda7-04cdd1a5812d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/LLM\n",
        "%ls"
      ],
      "metadata": {
        "id": "GxHjUN8ZAPNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f59643-f7a0-4488-d1da-d37a5203c938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/LLM\n",
            "attention_comp.ipynb      gpt_model.png                  sliding_window_sampling.py  training.ipynb\n",
            "attention_comp.py         \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                   the-verdict.txt\n",
            "gpt_implementation.ipynb  pytorch_wormup.ipynb           token_embedding.ipynb\n",
            "gpt_implementation.py     sliding_window_sampling.ipynb  tokenizing_text.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLGf4yhxO2lM",
        "outputId": "6728f834-6147-4519-f262-c87c0a04f981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.4/1.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! jupyter nbconvert --to python gpt_implementation.ipynb"
      ],
      "metadata": {
        "id": "YwB6vnQNAoUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6a0161-7aff-42df-9093-12c926f4fc83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook gpt_implementation.ipynb to python\n",
            "[NbConvertApp] Writing 18539 bytes to gpt_implementation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! jupyter nbconvert --to python sliding_window_sampling.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbQjwgFsO6YF",
        "outputId": "8455d997-95cc-4e98-d929-6205cb45d190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook sliding_window_sampling.ipynb to python\n",
            "[NbConvertApp] Writing 4643 bytes to sliding_window_sampling.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False}"
      ],
      "metadata": {
        "id": "9xEFmzluAz-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_implementation import GPTModel\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "9SXDBgTiU0yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "66St7yxZPh41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Generation"
      ],
      "metadata": {
        "id": "E5AnCKwjMGRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three-step text generation process:\n",
        "1. Convert input text into a series of token IDs\n",
        "2. Model generates coresponding logits, probability distribution for each token in the vocabulary\n",
        "3. Convert logits back into token IDs, decoded with tokenizer into text"
      ],
      "metadata": {
        "id": "PlTanU-8M7w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utility functions"
      ],
      "metadata": {
        "id": "0JY9R52sMR17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from gpt_implementation import generate_text_simple\n",
        "\n",
        "def text_to_token_ids(text, tokenizer, device=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n"
      ],
      "metadata": {
        "id": "YKrNN8RyMWIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text into tokens\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer, device=None),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(f\"Input text:\\n{start_context}\")\n",
        "print(f\"\\nOutput text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
      ],
      "metadata": {
        "id": "OssBKAmiWk62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6bc91f-c997-4031-f709-ea580c41af52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text:\n",
            "Every effort moves you\n",
            "\n",
            "Output text:\n",
            "Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_token_ids(start_context, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wMBrmO1rxaz",
        "outputId": "3fa61e9b-5f6b-481a-df7c-cb827858a7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6109, 3626, 6100,  345]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"every effort moves you\"\n",
        "text2 = \"I really like learning\"\n",
        "\n",
        "text1_tokens = text_to_token_ids(text1, tokenizer)\n",
        "text2_tokens = text_to_token_ids(text2, tokenizer)\n",
        "\n",
        "print(text1_tokens)\n",
        "print(text2_tokens)\n",
        "\n",
        "#inputs = torch.cat((text1_tokens, text2_tokens), 0)\n",
        "inputs = torch.tensor([[16833,  3626,  6100],\n",
        "                       [   40,  1107,   588]])\n",
        "print(f\"input tokens:\\n{inputs}\")\n",
        "targets =  torch.tensor([[3626,  6100,   345],\n",
        "                         [1107,   588,  4673]])\n",
        "# output logit\n",
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "# Probability of each token in vocabulary\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "print(f\"porbabilitiy scores tensor size:{probas.shape}\")"
      ],
      "metadata": {
        "id": "M-Hj96nNW3ss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1c4ae5-a026-4b0d-e8c4-ee36d204a9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[16833,  3626,  6100,   345]])\n",
            "tensor([[  40, 1107,  588, 4673]])\n",
            "input tokens:\n",
            "tensor([[16833,  3626,  6100],\n",
            "        [   40,  1107,   588]])\n",
            "porbabilitiy scores tensor size:torch.Size([2, 3, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from probabilitiy to token ids to text\n",
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"Token IDs:\\n\", token_ids)"
      ],
      "metadata": {
        "id": "4_xv9Xj-ZC0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f850277-1a6e-4f9e-84bf-f0f04034f585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[16657],\n",
            "         [  339],\n",
            "         [42826]],\n",
            "\n",
            "        [[49906],\n",
            "         [29669],\n",
            "         [41751]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"The 3 target ID probabilities for first batch:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"The 3 target ID probabilities for second batch:\", target_probas_2)"
      ],
      "metadata": {
        "id": "Tj95nsf20Szr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cb2351-d8ec-4f2c-cd5e-4914b0d70eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 3 target ID probabilities for first batch: tensor([    0.0001,     0.0000,     0.0000])\n",
            "The 3 target ID probabilities for second batch: tensor([    0.0000,     0.0001,     0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPT-2 vocabulary has 50257 tokens, so most of the initial probabilites will be around 1/50257 = 0.00002"
      ],
      "metadata": {
        "id": "ub5dbFy23qeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_idx = 0\n",
        "print(targets[text_idx])\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Initial softmax pobability score:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "print(targets[text_idx])\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Initial softmax probability score:\", target_probas_2)"
      ],
      "metadata": {
        "id": "iMlhjeFE4I9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a55fe3-dbff-4405-abdd-6dea292cfc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3626, 6100,  345])\n",
            "Initial softmax pobability score: tensor([    0.0001,     0.0000,     0.0000])\n",
            "tensor([1107,  588, 4673])\n",
            "Initial softmax probability score: tensor([    0.0000,     0.0001,     0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Computation"
      ],
      "metadata": {
        "id": "w1v0278vPZmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text evalutaion measures how far the generated tokens are from the targets. The model training aims to increase the softmax probability used in the evaluation metric. Calculating the loss requires\n",
        "\n",
        "1. Logits\n",
        "2. Probabilities\n",
        "3. Target probabilities\n",
        "4. Log probabilities\n",
        "5. Average log probability\n",
        "6. Cross Entropy\n",
        "\n",
        "The goal is to push the average log probabilitiy as close to 0, which is equivalent to bring the negative average log probability down to 0 with training and updating the model weights. Cross entropy computes a mesure for discreate outcomes the true distribution of takens and the token probabilities generated by LLM.\n",
        "\n",
        "Perplexity is exp(cross entropy) signifies the effective vocabulary size obout which the model is uncertain at each step.\n"
      ],
      "metadata": {
        "id": "UpKkTvq7yTBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "\n",
        "print(f\"Log of probabilitiey scores:\\n{log_probas}\")"
      ],
      "metadata": {
        "id": "0BAusmbl3HA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da08798-cfa4-4b36-c2e4-d97043fbc807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log of probabilitiey scores:\n",
            "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -11.6089])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_log_probas = torch.mean(log_probas)\n",
        "\n",
        "print(f\"Combine log probabilities into a single score:\\n{avg_log_probas}\")"
      ],
      "metadata": {
        "id": "Ngy-SeQr6Rjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf762fd-16dd-4f9f-cec1-5481180c2aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combine log probabilities into a single score:\n",
            "-10.686089515686035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(f\"Negative average log probability:\\n{neg_avg_log_probas}\")"
      ],
      "metadata": {
        "id": "VRf1oVa0dvR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b6977d-1a02-4273-be8b-388ffbb26db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative average log probability:\n",
            "10.686089515686035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logits shape:\", logits.shape)\n",
        "print(\"Targets shape:\", targets.shape)"
      ],
      "metadata": {
        "id": "tKoUU_wngvda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496eff64-2187-45a3-8c26-6e1923bce19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 3, 50257])\n",
            "Targets shape: torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "print(\"Flattened logits:\", logits_flat.shape)\n",
        "print(\"Flattened targets:\", targets_flat.shape)"
      ],
      "metadata": {
        "id": "QVfYGOpuhPtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1b118a-dc52-46f3-939c-ecaffce8d41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened logits: torch.Size([6, 50257])\n",
            "Flattened targets: torch.Size([6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(f\"PyTorch cross_entropy:\\n{loss}\")"
      ],
      "metadata": {
        "id": "6dRowZxbh9qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598d86ed-6268-4d6a-e303-0e257f2a6d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch cross_entropy:\n",
            "10.686089515686035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = torch.exp(loss)\n",
        "print(f\"Perplexity of\\n{perplexity}\\nis the number of words in the vocabulary the\\nmodel has being unsure to generate as the next token\")"
      ],
      "metadata": {
        "id": "Ri2ZHvTLilSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfc6eb7-e580-4faf-d990-c933ed0f3ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of\n",
            "43743.11328125\n",
            "is the number of words in the vocabulary the\n",
            "model has being unsure to generate as the next token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation losses"
      ],
      "metadata": {
        "id": "sV2YtOUmHYT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the loss on the training and validation split of the short story \"The Verdict\""
      ],
      "metadata": {
        "id": "MyjeBgtZHtBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import re\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# load the dataset\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  text_data = file.read()\n",
        "\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Total number of characters:\", total_characters)\n",
        "print(\"Total number of words:\", len(text_data.split()))\n",
        "print(\"Total number of tokens:\", total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DO4H_m3ICem",
        "outputId": "579935c5-5b03-4326-fcfd-b8bceb765535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20480\n",
            "Total number of words: 3634\n",
            "Total number of tokens: 5146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide the data into trainig and validation\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "print(\"length of training data:\",len(train_data))\n",
        "print(\"length of validation data:\",len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUM96ZOYI4bx",
        "outputId": "b5a19fba-b7ca-4264-8b87-ea24b6abf3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of training data: 18432\n",
            "length of validation data: 2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create training and validation data loaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sliding_window_sampling\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = sliding_window_sampling.dataloader(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    shuffle=True)\n",
        "\n",
        "val_loader = sliding_window_sampling.dataloader(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    shuffle=False)"
      ],
      "metadata": {
        "id": "9zn-iCM8P8z0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d8f1aa-c53e-4263-860d-ca1ab724be54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "tiktoken version: 0.6.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "attention_comp.ipynb      gpt_model.png                  sliding_window_sampling.py  training.ipynb\n",
            "attention_comp.py         \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                   the-verdict.txt\n",
            "gpt_implementation.ipynb  pytorch_wormup.ipynb           token_embedding.ipynb\n",
            "gpt_implementation.py     sliding_window_sampling.ipynb  tokenizing_text.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/LLM\n",
            "attention_comp.ipynb      gpt_model.png                  sliding_window_sampling.py  training.ipynb\n",
            "attention_comp.py         \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                   the-verdict.txt\n",
            "gpt_implementation.ipynb  pytorch_wormup.ipynb           token_embedding.ipynb\n",
            "gpt_implementation.py     sliding_window_sampling.ipynb  tokenizing_text.ipynb\n",
            "5146\n",
            "encoded text sample:\n",
            "[40, 367, 2885, 1464, 1807, 3619, 402, 271, 10899, 2138, 257, 7026, 15632, 438, 2016, 257, 922, 5891, 1576, 438, 568, 340, 373, 645, 1049, 5975, 284, 502, 284, 3285, 326, 11, 287, 262, 6001, 286, 465, 13476, 11, 339, 550, 5710, 465, 12036, 11, 6405, 257, 5527, 27075, 11]\n",
            "x: [40, 367, 2885, 1464]\n",
            "y:      [367, 2885, 1464, 1807]\n",
            "[40] ----> 367\n",
            "[40, 367] ----> 2885\n",
            "[40, 367, 2885] ----> 1464\n",
            "[40, 367, 2885, 1464] ----> 1807\n",
            "I ---->  H\n",
            "I H ----> AD\n",
            "I HAD ---->  always\n",
            "I HAD always ---->  thought\n",
            "One batch:\n",
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [  257,  7026, 15632,   438],\n",
            "        [  257,   922,  5891,  1576],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [  326,    11,   287,   262],\n",
            "        [  286,   465, 13476,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [  402,   271, 10899,  2138],\n",
            "        [ 7026, 15632,   438,  2016],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [  284,   502,   284,  3285],\n",
            "        [   11,   287,   262,  6001],\n",
            "        [  465, 13476,    11,   339]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtkjRnrtRTt6",
        "outputId": "1fbf7d9c-df62-4619-cbb7-459ff20848c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_loss(input_batch, target_batch, model, device):\n",
        "  \"\"\" Utility function which calculates the cross entropy loss of a single batch\"\"\"\n",
        "\n",
        "  input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(\n",
        "        logits.flatten(0, 1), target_batch.flatten())\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "heaNDjszSED8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loader_loss(data_loader, model, device, num_batches=None):\n",
        "  \"\"\" Computes the loss over all the batches in a data loader.\n",
        "      It iterates over all batches in a data loader, but can\n",
        "      speed up the evaluation with a smaller number num_batches. \"\"\"\n",
        "\n",
        "  total_loss = 0.\n",
        "\n",
        "  if num_batches is None:\n",
        "      num_batches = len(data_loader)\n",
        "  else:\n",
        "      num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "      if i < num_batches:\n",
        "          loss = batch_loss(input_batch, target_batch, model, device)\n",
        "          total_loss += loss.item()\n",
        "      else:\n",
        "          break\n",
        "\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "pST7YsOjStmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_loss = loader_loss(train_loader, model, device)\n",
        "val_loss = loader_loss(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNZvx9yGUKjy",
        "outputId": "61a4d2a0-db11-4858-88da-98dcc8265da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987583796183268\n",
            "Validation loss: 10.983160018920898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The loss values are high beacuse the model has not yet been trained."
      ],
      "metadata": {
        "id": "wvXXYzQvewuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "cuN417xVN5gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training consists in iterating over the batches for some number of epochs. For each batch the loss is calculated, loss gradients are computed with backward pass, model weights are updated, the training and validation losses are printed and at the end of each epoch a sample text is generated."
      ],
      "metadata": {
        "id": "fKmoHtnoOLvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  \"\"\" Function to set the model in evaluation mode, disabling gradient tracking\n",
        "      and dropout, computes training and validation losses for a\n",
        "      specified eval frequency.\"\"\"\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      train_loss = loader_loss(train_loader, model, device, num_batches=eval_iter)\n",
        "      val_loss = loader_loss(val_loader, model, device, num_batches=eval_iter)\n",
        "  model.train()\n",
        "  return train_loss, val_loss"
      ],
      "metadata": {
        "id": "ZYSN80pjN-EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  \"\"\" Function to track if the model improves during training. It takes in\n",
        "      a start_context text, converts it into token IDs and feeds it into\n",
        "      the LLM to generate a text sample using generate_text_simple function.\"\"\"\n",
        "  # turn on evaluation mode\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "\n",
        "  # a context manager, leaving with block the gradient computation is turned on\n",
        "  with torch.no_grad():\n",
        "      token_ids = generate_text_simple(model=model,\n",
        "                                       idx=encoded,\n",
        "                                       max_new_tokens=50,\n",
        "                                       context_size=context_size)\n",
        "\n",
        "      decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "      print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "\n",
        "  # turn off evaluation model: normalization layer uses per batch statistics\n",
        "  # dropout layer is activated\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "3HRl4-zYOGMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context):\n",
        "    \"\"\" Function that implements the training flow: iterating over each epoch,\n",
        "        processing batches, resetting and calculating gradients, updating weights,\n",
        "        and monitoring with printing losses and generating text samples. \"\"\"\n",
        "\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = batch_loss(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        generate_and_print_sample(\n",
        "            model, train_loader.dataset.tokenizer, device, start_context\n",
        "        )\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "XHW4n2VBOJhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the GPTModel instance\n",
        "torch.manual_seed(123)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "model.to(device)\n",
        "# AdamW optimizer improves the weight decay, penalizes larger weights and\n",
        "# prevents overfitting\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
        "    start_context=\"Every effort moves you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS3HDf66J49B",
        "outputId": "a9d797da-dea8-445f-c619-6066f02251c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 10.061, Val loss 9.932\n",
            "Ep 1 (Step 000005): Train loss 8.156, Val loss 8.338\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.553, Val loss 7.055\n",
            "Ep 2 (Step 000015): Train loss 5.922, Val loss 6.601\n",
            "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
            "Ep 3 (Step 000020): Train loss 5.814, Val loss 6.505\n",
            "Ep 3 (Step 000025): Train loss 5.366, Val loss 6.383\n",
            "Every effort moves you, and to the of the of the of the, and I had. Gis, and, and, and, and, and, and the, and, and, and, and, and, and, and, and, and, and\n",
            "Ep 4 (Step 000030): Train loss 4.609, Val loss 6.287\n",
            "Ep 4 (Step 000035): Train loss 5.019, Val loss 6.297\n",
            "Every effort moves you, I had the of the of the picture to the picture.               \"I\"I had the  \"I the           \n",
            "Ep 5 (Step 000040): Train loss 4.147, Val loss 6.178\n",
            "Every effort moves you know the                                                \n",
            "Ep 6 (Step 000045): Train loss 3.994, Val loss 6.153\n",
            "Ep 6 (Step 000050): Train loss 2.866, Val loss 6.145\n",
            "Every effort moves you know the fact, and pushed one of the to the fact with a little a little to my work, and in fact, and.         \"Oh, and he was, and down the room, and in\n",
            "Ep 7 (Step 000055): Train loss 3.102, Val loss 6.183\n",
            "Ep 7 (Step 000060): Train loss 2.096, Val loss 6.130\n",
            "Every effort moves you know,\" was not that the picture for a smile that he had the last word.        \"I looked. \"--as Jack himself at my elbow and he was a little a--because he was his pictures\n",
            "Ep 8 (Step 000065): Train loss 1.970, Val loss 6.148\n",
            "Ep 8 (Step 000070): Train loss 1.373, Val loss 6.222\n",
            "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"I looked up at the!  \"I looked up the moment--as Jack himself, as once one had been the donkey. \"There were days when I\n",
            "Ep 9 (Step 000075): Train loss 1.223, Val loss 6.228\n",
            "Ep 9 (Step 000080): Train loss 0.840, Val loss 6.256\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with the last word. He was no great, and in an unusual degree to the display of his close grayish beard--as if he had the donkey. \"There were days when I\n",
            "Ep 10 (Step 000085): Train loss 0.590, Val loss 6.365\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visulaize training and validation losses\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "k2gWMo73OOxP",
        "outputId": "45fea6b0-8a54-4e18-ce37-678463ee1bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The  Training and Validation losses start to diverge which means that the model is overfitting.The model memorizes the training data by searching for the generated text like \"quite insensible to the irony\" found in the \"The Verdict\" text. The small training dataset and the multiple epochs lead to model memorization. Sampling methods used by LLMs mitigates it."
      ],
      "metadata": {
        "id": "ZhehtlrDOn3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model parameters with state_dict mapping each layer to its parameters\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "dfynnVlfkhyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model and the adaptive optimizer AdamW stored additional parameters\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "h4IIbgBaiLRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to restore the model need to load the saved params and the use load_state_dict\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train()"
      ],
      "metadata": {
        "id": "YQbLYhCXjXBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}