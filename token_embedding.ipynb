{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl6E6cB++YLtXnRX8sn526",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliemci/building-LLM/blob/main/token_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token ID Embeding"
      ],
      "metadata": {
        "id": "7LMIT-21qcoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion of words to continuous vector representation"
      ],
      "metadata": {
        "id": "be5Yo0XZqq_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0mCROHHprlf",
        "outputId": "8bcffc56-d652-4426-99fb-c1476eb69af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/LLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsvjAy4upxFr",
        "outputId": "da289a5d-c168-4084-fb5f-3b7ebfb74f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cc9uL3svJVX",
        "outputId": "e9afda9a-c018-446d-c7ab-78c538998eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch_wormup.ipynb           the-verdict.txt        tokenizing_text.ipynb\n",
            "sliding_window_sampling.ipynb  token_embedding.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "W82SvO3y0RhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b031093-e234-46be-c73d-66ba21261578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "Z5CvNkHQqG3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize"
      ],
      "metadata": {
        "id": "fjFUT_C-zxga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate BPE tokenizer from tiktoken, which BPE builds its vocabulary by iteratively\n",
        "# merging frequent characters into subwords and frequent subwords into words\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "YSf6M5i1qLzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "id": "038R1mShpyLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b131967-c930-4f7d-b5f9-a04ebd45369e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset and Data Loder"
      ],
      "metadata": {
        "id": "aaCWIMVuz2t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "  \"\"\" Extends PyTorch Dataset class and defines how individual\n",
        "      rows are fetched from the dataset, witch each row constiting\n",
        "      of a number of token IDs max_length + 1. \"\"\"\n",
        "\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "lXu7bWdQ05rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataloader(text, batch_size=4, max_length=256, stride=128, shuffle=True):\n",
        "  \"\"\" A PyTorch DataLoader which uses the dataset generated by DatasetV1\n",
        "      to load the inputs in batches.\"\"\"\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = Dataset(text, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader( dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "VbT_qIryz8ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = dataloader(raw_text, batch_size=8, max_length=4, stride=5, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "second_batch = next(data_iter)\n",
        "\n",
        "print(f\"first batch:\\n{first_batch}\")\n",
        "print(f\"\\nsecond batch:\\n{second_batch}\")"
      ],
      "metadata": {
        "id": "PywhykMb1hgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6d02aa-2f57-45bf-aee8-eeb31bd70222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first batch:\n",
            "[tensor([[   40,   367,  2885,  1464],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [  257,  7026, 15632,   438],\n",
            "        [  257,   922,  5891,  1576],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [  326,    11,   287,   262],\n",
            "        [  286,   465, 13476,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
            "        [  402,   271, 10899,  2138],\n",
            "        [ 7026, 15632,   438,  2016],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [  284,   502,   284,  3285],\n",
            "        [   11,   287,   262,  6001],\n",
            "        [  465, 13476,    11,   339]])]\n",
            "\n",
            "second batch:\n",
            "[tensor([[  550,  5710,   465, 12036],\n",
            "        [ 6405,   257,  5527, 27075],\n",
            "        [  290,  4920,  2241,   287],\n",
            "        [ 4489,    64,   319,   262],\n",
            "        [41976,    13,   357, 10915],\n",
            "        [ 2138,  1807,   340,   561],\n",
            "        [  587, 10598,   393, 28537],\n",
            "        [  198,   198,     1,   464]]), tensor([[ 5710,   465, 12036,    11],\n",
            "        [  257,  5527, 27075,    11],\n",
            "        [ 4920,  2241,   287,   257],\n",
            "        [   64,   319,   262, 34686],\n",
            "        [   13,   357, 10915,   314],\n",
            "        [ 1807,   340,   561,   423],\n",
            "        [10598,   393, 28537,  2014],\n",
            "        [  198,     1,   464,  6001]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position independent embedding"
      ],
      "metadata": {
        "id": "TIseyjRGrC5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeding layer converts the token ID into a continuous vector representation where the same token ID always gets mapped to the same vector, regarless of where the token ID is positioned in the input sequence. <br>\n",
        "**Note:** The self-attention mechanism is position-agnostic <br><br>\n",
        "\n",
        "Create a emebdding of size 250 for 50257 words, instead of BPF tokenizer vocabulary of 50257 words in GPT-3 emebding size of 12288."
      ],
      "metadata": {
        "id": "LhMCTUwe3d5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "# max length of input text\n",
        "max_length = 4\n",
        "# suported input size of the LLM\n",
        "context_length = max_length\n",
        "\n",
        "# the input text is truncated if max_length > context_length"
      ],
      "metadata": {
        "id": "tJwQgjH7rNfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the embeding layer in PyTorch with small random values weight matrix\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# embed each tocken of each batch into a output_dim - dimensional vector\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(f\"randomly initialized weight matrix:\\n{embedding_layer.weight}\")"
      ],
      "metadata": {
        "id": "0_x0T37yrNY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da44f2e3-1260-47ef-99cf-603c33cfb96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "randomly initialized weight matrix:\n",
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.3035,  ...,  1.3337,  0.0771, -0.0522],\n",
            "        [ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
            "        [-1.3152, -0.0677, -0.1350,  ..., -0.3181, -1.3936,  0.5226],\n",
            "        ...,\n",
            "        [ 0.5871, -0.0572, -1.1628,  ..., -0.6887, -0.7364,  0.4479],\n",
            "        [ 0.4438,  0.7411,  1.1263,  ...,  1.2091,  0.6781,  0.3331],\n",
            "        [-0.2537,  0.1446,  0.7203,  ..., -0.2134,  0.2144,  0.3006]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
        "\n",
        "The embeding vectors values are optimized during LLM training as part of the LLM optimization."
      ],
      "metadata": {
        "id": "OEWD0yBc49AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of converting a single token ID of 1 into a 256-dim embedding vector\n",
        "print(embedding_layer(torch.tensor([1])))"
      ],
      "metadata": {
        "id": "w1KQiY16rNDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3266b145-76bb-4e91-ea52-216068ca4602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2386,  0.1411, -1.3354, -2.9340,  0.1141, -1.2072, -0.3008,  0.1427,\n",
            "         -1.3027, -0.4919, -2.1429,  0.9488, -0.5684, -0.0646,  0.6647, -2.7836,\n",
            "          1.1366,  0.9089,  0.9494,  0.0266, -0.9221,  0.7034, -0.3659, -0.1965,\n",
            "         -0.9207,  0.3154, -0.0217,  0.3441,  0.2271, -0.4597, -0.6183,  0.2461,\n",
            "         -0.4055, -0.8368,  1.2277, -0.4297, -2.2121, -0.3780,  0.9838, -1.0895,\n",
            "          0.2017,  0.0221, -1.7753, -0.7490,  0.2781, -0.9621, -0.4223, -1.1036,\n",
            "          0.2473,  1.4549, -0.2835, -0.3767, -0.0306, -0.0894, -0.1965, -0.9713,\n",
            "          0.9005, -0.2523,  1.0669, -0.2985,  0.8558,  1.6098, -1.1893,  1.1677,\n",
            "          0.3277, -0.8331, -1.6179,  0.2265, -0.4382,  0.3265, -1.5786, -1.3995,\n",
            "          0.5446, -0.0830, -1.1753,  1.7825,  1.7524, -0.2135,  0.4095,  0.0465,\n",
            "          0.6367, -0.1943, -0.8614,  0.5338,  0.9376, -0.9225,  0.7047, -0.2722,\n",
            "          0.0144, -0.6411,  2.3902, -1.4256, -0.4619, -1.5539, -0.3338,  0.2405,\n",
            "          2.1065,  0.5509, -0.2936, -1.8027, -0.6933,  1.7409,  0.2698,  0.9595,\n",
            "         -1.0253, -0.5505,  1.0264, -0.5670, -0.2658, -1.1116, -1.3696, -0.6534,\n",
            "         -1.6125, -0.2284,  1.8388, -0.9473,  0.1419,  0.3696, -0.0174, -0.9575,\n",
            "         -0.8169, -0.2866,  0.4343, -0.1340, -2.1467, -1.7984, -0.6822, -0.5191,\n",
            "          0.0093, -1.8110, -0.2443,  0.1327,  1.0875, -0.1029,  0.8604,  0.2078,\n",
            "          0.2027,  0.5021, -0.4063,  0.6664,  0.4765, -1.4498,  1.5446,  1.0394,\n",
            "          2.1681,  0.4884,  0.3359, -1.2282, -0.1200,  0.4884,  1.9431,  0.2169,\n",
            "         -0.4743, -0.3679, -0.2918, -1.6531,  0.7692, -1.1323,  2.9590,  0.8171,\n",
            "          0.7668,  1.3258,  0.2103,  1.7876, -1.2128,  0.2045,  1.1051, -0.5454,\n",
            "          0.1073,  0.8727, -1.2800, -0.4619,  1.4342, -1.2103,  1.3834,  0.0324,\n",
            "          0.5421,  0.8796,  0.2713,  1.6067, -1.0004,  0.7392, -0.4931,  0.4073,\n",
            "         -1.0394, -0.3226,  0.7226,  0.2674, -0.4673,  0.6916, -1.8752,  0.3008,\n",
            "         -0.1468,  1.3672,  0.7074,  0.3276,  1.0658,  1.4130, -1.2445,  0.2227,\n",
            "          0.4593, -0.3845,  0.6554, -0.1045, -1.1134,  0.5110,  0.3566,  1.8591,\n",
            "         -0.9300,  1.1186,  1.7495,  2.3058,  0.3734,  0.3314, -0.1871,  0.1770,\n",
            "          2.9641,  0.2307,  0.3228,  0.2610,  0.3219,  1.7745,  0.3155, -0.9364,\n",
            "          0.5687, -0.0959,  0.0046, -1.4321, -0.1535, -0.1925, -0.3115, -0.1812,\n",
            "         -0.8745, -0.0270,  0.5424,  1.3656, -0.0284, -0.7411, -0.0169,  1.7024,\n",
            "          0.4206,  0.9317,  0.9884, -0.3948,  0.6919,  1.2310, -0.5126, -1.2635,\n",
            "          1.1440,  0.7619,  0.6543, -1.5402, -0.5176, -0.0315, -1.0640,  0.9417]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the instantiated embedding layer to 4 input token IDs to obtain embedding vectors\n",
        "token_embeddings = embedding_layer(first_batch[0])\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "id": "kl3i2XjgrNS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e4b644-a859-4ab3-bf28-6a135405c39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-6.3964e-02,  3.3174e-01,  1.0698e-01,  ...,  5.3491e-01,\n",
            "          -8.0244e-01, -2.3238e+00],\n",
            "         [-3.5248e-01,  3.5087e-01,  9.8728e-01,  ..., -1.8466e+00,\n",
            "          -1.7034e+00,  3.2226e-01],\n",
            "         [ 1.0017e+00,  9.2986e-01, -1.2633e+00,  ..., -1.2256e+00,\n",
            "           1.1179e+00,  1.3427e-01],\n",
            "         [ 7.9961e-01,  2.2837e+00, -6.5249e-01,  ..., -1.1217e+00,\n",
            "           4.7057e-01,  1.5314e-01]],\n",
            "\n",
            "        [[-2.4273e-01,  9.1447e-01,  1.0885e+00,  ..., -8.6509e-01,\n",
            "           3.5269e+00,  7.2247e-01],\n",
            "         [-5.4342e-01,  1.6203e+00,  1.2222e+00,  ...,  6.8139e-01,\n",
            "          -1.4032e+00,  1.4922e-01],\n",
            "         [-3.5035e-01, -9.3247e-01, -1.2900e+00,  ..., -1.4980e+00,\n",
            "           1.3996e-01,  3.7301e-01],\n",
            "         [ 6.7677e-02, -1.0757e+00,  1.3706e-01,  ..., -3.9116e-01,\n",
            "          -1.0974e+00,  7.5906e-01]],\n",
            "\n",
            "        [[ 1.1902e-01,  7.6004e-01, -9.3059e-01,  ...,  6.4696e-04,\n",
            "           8.6058e-01, -1.3698e+00],\n",
            "         [ 9.6366e-03,  5.7844e-01,  3.1312e-01,  ..., -1.3150e-01,\n",
            "          -1.1338e+00, -1.1401e+00],\n",
            "         [ 1.3577e+00,  5.4661e-01,  3.5129e-02,  ..., -3.3581e-01,\n",
            "           1.1221e+00,  2.7318e-01],\n",
            "         [ 3.5463e+00, -6.0930e-01,  3.7334e-01,  ...,  7.1445e-01,\n",
            "           1.0079e+00, -8.4482e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4425e+00,  1.5972e+00, -4.8457e-03,  ...,  6.1644e-01,\n",
            "          -1.8011e+00,  9.1017e-01],\n",
            "         [-2.7693e+00, -1.0681e+00,  1.7515e+00,  ...,  1.4617e-01,\n",
            "          -2.5560e+00,  2.2617e+00],\n",
            "         [-1.1585e+00, -2.3485e+00, -7.0867e-01,  ...,  1.1681e+00,\n",
            "          -7.7105e-01, -4.2509e-01],\n",
            "         [-2.7693e+00, -1.0681e+00,  1.7515e+00,  ...,  1.4617e-01,\n",
            "          -2.5560e+00,  2.2617e+00]],\n",
            "\n",
            "        [[-4.5952e-01, -1.1465e-01, -2.0506e-01,  ...,  1.2356e+00,\n",
            "          -9.5095e-01, -2.9712e-01],\n",
            "         [ 1.8056e+00, -1.0064e+00,  1.5822e-01,  ...,  2.3792e-01,\n",
            "          -1.1839e+00, -3.1790e-01],\n",
            "         [-1.5235e-02,  1.3879e+00,  2.3367e-01,  ..., -1.5514e+00,\n",
            "           3.5856e+00,  1.5642e+00],\n",
            "         [ 2.7836e-02, -1.4592e+00,  1.7823e-01,  ..., -1.7335e+00,\n",
            "           9.3458e-01, -2.4222e-01]],\n",
            "\n",
            "        [[ 2.3164e+00,  7.2268e-01,  7.6435e-02,  ..., -1.3414e+00,\n",
            "          -1.8928e-01,  5.4432e-01],\n",
            "         [-8.0272e-01, -1.8098e+00,  5.5493e-01,  ..., -4.0198e-01,\n",
            "           1.4956e-01,  2.5378e-01],\n",
            "         [ 2.7100e+00, -7.5557e-01,  2.2625e-01,  ...,  5.1386e-01,\n",
            "           1.6719e+00,  6.6725e-01],\n",
            "         [ 1.8056e+00, -1.0064e+00,  1.5822e-01,  ...,  2.3792e-01,\n",
            "          -1.1839e+00, -3.1790e-01]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "jOjZt7ONrGny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional embedding understands the order and relationships between tokens, ensuring context-aware predictions.<br>\n",
        "\n",
        "Position information is achived through relative and absolute positional embeddings.<br><br>\n",
        "**Absolute positional embeddings** for each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location.<br>\n",
        "*OpenAI's GTP model* uses absolute positional embeddings that are optimize during the training process.\n"
      ],
      "metadata": {
        "id": "oGKBMTcJG3tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Absolute Embedding"
      ],
      "metadata": {
        "id": "XAl_tdZjL7Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create another embedding layer that has the same dimension as the token_embedding_layer, a placeholder vector which contains a sequence of numbers 0,1, ... , up to the maximum input length - 1"
      ],
      "metadata": {
        "id": "ZIC9eX8IMCWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "print(f\"Size of posisiton embedindg tensor: {pos_embeddings.shape}\")\n",
        "print(f\"Posisiton embedindg tensor: {pos_embeddings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKKrdbX3JOHQ",
        "outputId": "c0bf79e7-6f89-4627-f4b0-8dfff024a2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of posisiton embedindg tensor: torch.Size([4, 256])\n",
            "Posisiton embedindg tensor: tensor([[-0.9694,  0.4167,  0.5280,  ..., -0.0023,  1.1440,  0.8301],\n",
            "        [-1.4067, -0.8280, -0.3587,  ..., -0.9408,  1.5647, -0.6394],\n",
            "        [ 0.3999, -0.3997, -1.9166,  ...,  0.1630,  0.2393, -0.1784],\n",
            "        [-0.5676,  1.7856, -0.0915,  ..., -0.7356,  1.2118, -1.1895]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add the positional embedding tensor to the token embeddings"
      ],
      "metadata": {
        "id": "N7IAKb3INCmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "\n",
        "print(f\"Size of input embedding tensor for the first row of the dataset:\\n{input_embeddings.shape}\")\n",
        "print(f\" Input embeddings:\\n{input_embeddings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL6HWOHYOilH",
        "outputId": "06fb8412-126f-4eba-bbfc-63535937821e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of input embedding tensor for the first row of the dataset:\n",
            "torch.Size([8, 4, 256])\n",
            " Input embeddings:\n",
            "tensor([[[-1.0334e+00,  7.4843e-01,  6.3500e-01,  ...,  5.3259e-01,\n",
            "           3.4160e-01, -1.4937e+00],\n",
            "         [-1.7592e+00, -4.7717e-01,  6.2860e-01,  ..., -2.7873e+00,\n",
            "          -1.3879e-01, -3.1713e-01],\n",
            "         [ 1.4016e+00,  5.3016e-01, -3.1799e+00,  ..., -1.0627e+00,\n",
            "           1.3572e+00, -4.4152e-02],\n",
            "         [ 2.3202e-01,  4.0693e+00, -7.4397e-01,  ..., -1.8572e+00,\n",
            "           1.6823e+00, -1.0363e+00]],\n",
            "\n",
            "        [[-1.2121e+00,  1.3312e+00,  1.6165e+00,  ..., -8.6741e-01,\n",
            "           4.6709e+00,  1.5526e+00],\n",
            "         [-1.9501e+00,  7.9228e-01,  8.6353e-01,  ..., -2.5938e-01,\n",
            "           1.6145e-01, -4.9017e-01],\n",
            "         [ 4.9515e-02, -1.3322e+00, -3.2066e+00,  ..., -1.3350e+00,\n",
            "           3.7927e-01,  1.9458e-01],\n",
            "         [-4.9991e-01,  7.0983e-01,  4.5585e-02,  ..., -1.1267e+00,\n",
            "           1.1434e-01, -4.3041e-01]],\n",
            "\n",
            "        [[-8.5037e-01,  1.1767e+00, -4.0257e-01,  ..., -1.6790e-03,\n",
            "           2.0046e+00, -5.3969e-01],\n",
            "         [-1.3971e+00, -2.4959e-01, -4.5557e-02,  ..., -1.0723e+00,\n",
            "           4.3082e-01, -1.7795e+00],\n",
            "         [ 1.7575e+00,  1.4691e-01, -1.8815e+00,  ..., -1.7282e-01,\n",
            "           1.3614e+00,  9.4753e-02],\n",
            "         [ 2.9787e+00,  1.1763e+00,  2.8186e-01,  ..., -2.1127e-02,\n",
            "           2.2197e+00, -2.0343e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.4119e+00,  2.0139e+00,  5.2318e-01,  ...,  6.1411e-01,\n",
            "          -6.5702e-01,  1.7403e+00],\n",
            "         [-4.1760e+00, -1.8962e+00,  1.3928e+00,  ..., -7.9459e-01,\n",
            "          -9.9133e-01,  1.6223e+00],\n",
            "         [-7.5860e-01, -2.7482e+00, -2.6252e+00,  ...,  1.3311e+00,\n",
            "          -5.3175e-01, -6.0351e-01],\n",
            "         [-3.3369e+00,  7.1743e-01,  1.6600e+00,  ..., -5.8940e-01,\n",
            "          -1.3442e+00,  1.0722e+00]],\n",
            "\n",
            "        [[-1.4289e+00,  3.0203e-01,  3.2296e-01,  ...,  1.2333e+00,\n",
            "           1.9309e-01,  5.3298e-01],\n",
            "         [ 3.9888e-01, -1.8345e+00, -2.0045e-01,  ..., -7.0284e-01,\n",
            "           3.8076e-01, -9.5729e-01],\n",
            "         [ 3.8463e-01,  9.8818e-01, -1.6829e+00,  ..., -1.3885e+00,\n",
            "           3.8249e+00,  1.3857e+00],\n",
            "         [-5.3975e-01,  3.2641e-01,  8.6748e-02,  ..., -2.4690e+00,\n",
            "           2.1463e+00, -1.4317e+00]],\n",
            "\n",
            "        [[ 1.3470e+00,  1.1394e+00,  6.0446e-01,  ..., -1.3438e+00,\n",
            "           9.5476e-01,  1.3744e+00],\n",
            "         [-2.2094e+00, -2.6379e+00,  1.9626e-01,  ..., -1.3427e+00,\n",
            "           1.7142e+00, -3.8561e-01],\n",
            "         [ 3.1098e+00, -1.1553e+00, -1.6903e+00,  ...,  6.7684e-01,\n",
            "           1.9112e+00,  4.8882e-01],\n",
            "         [ 1.2380e+00,  7.7912e-01,  6.6744e-02,  ..., -4.9765e-01,\n",
            "           2.7880e-02, -1.5074e+00]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relative Positional embeddings\n",
        "\n",
        "The model learns the relationships in terms of distance between tokens.This model can generalize better to sequences of varying lengths even if hasn't seen such lengths during training"
      ],
      "metadata": {
        "id": "AsUYASEiPMel"
      }
    }
  ]
}